{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "WERSJA ROBOCZA"
      ],
      "metadata": {
        "id": "waH0r8O4YNU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = VoiceDataset()\n",
        "dataset.csv_path = \"sciezka/do/pliku.csv\"\n",
        "dataset.data_path = [\"sciezka/do/audio1.wav\", \"sciezka/do/audio2.wav\"]\n",
        "dataset.return_metadata = True\n",
        "dataset.return_ground_truth = True\n",
        "dataset.raw_transforms = raw_transforms\n",
        "dataset.representation_transforms = representation_transforms\n",
        "dataset.ground_truth_mapper = {\"Healthy\": 0, \"Parkinson\": 1}\n"
      ],
      "metadata": {
        "id": "7cY-fsxNUnNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "import torchaudio.transforms as transforms\n",
        "\n",
        "def get_metadata(self, index):\n",
        "    metadata_df = pd.read_csv(self.csv_path)\n",
        "    row = metadata_df.iloc[index]\n",
        "    metadata = row.to_dict()\n",
        "    return metadata\n",
        "\n",
        "def get_ground_truth(self, index):\n",
        "    metadata_df = pd.read_csv(self.csv_path)\n",
        "    ground_truth = metadata_df.iloc[index][\"label\"]\n",
        "    return ground_truth\n",
        "\n",
        "class VoiceDataset(tc.utils.data.Dataset):\n",
        "    def __getitem__(self, index):\n",
        "        audio_file_path = self.data_path[index]\n",
        "        audio_data, sample_rate = sf.read(audio_file_path)\n",
        "\n",
        "        if self.raw_transforms is not None:\n",
        "            audio_data = self.raw_transforms(audio_data)\n",
        "\n",
        "        metadata = None\n",
        "        if self.return_metadata:\n",
        "            metadata = self.get_metadata(index)\n",
        "\n",
        "        ground_truth = None\n",
        "        if self.return_ground_truth:\n",
        "            ground_truth = self.get_ground_truth(index)\n",
        "\n",
        "        if self.representation_transforms is not None:\n",
        "            audio_data = self.representation_transforms(audio_data)\n",
        "\n",
        "        if self.ground_truth_mapper is not None and ground_truth is not None:\n",
        "            ground_truth = self.ground_truth_mapper.get(ground_truth, ground_truth)\n",
        "\n",
        "        if self.return_metadata and self.return_ground_truth:\n",
        "            return audio_data, metadata, ground_truth\n",
        "        elif self.return_metadata:\n",
        "            return audio_data, metadata\n",
        "        elif self.return_ground_truth:\n",
        "            return audio_data, ground_truth\n",
        "        else:\n",
        "            return audio_data\n",
        "\n",
        "\n",
        "def raw_transforms(audio_data, transform_type=\"normalize\", **kwargs):\n",
        "    if transform_type == \"normalize\":\n",
        "        normalized_audio = transforms.Normalize()(audio_data)\n",
        "        return normalized_audio\n",
        "\n",
        "    elif transform_type == \"filter\":\n",
        "        filtered_audio = transforms.LowpassFilter(**kwargs)(audio_data)\n",
        "        return filtered_audio\n",
        "\n",
        "    elif transform_type == \"change_volume\":\n",
        "        volume_changed_audio = transforms.Vol()(audio_data, **kwargs)\n",
        "        return volume_changed_audio\n",
        "\n",
        "    elif transform_type == \"time_stretch\":\n",
        "        time_stretched_audio = transforms.TimeStretch()(audio_data, **kwargs)\n",
        "        return time_stretched_audio\n",
        "\n",
        "    elif transform_type == \"spectrogram\":\n",
        "        waveform = torchaudio.tensor(audio_data)\n",
        "        spectrogram = transforms.Spectrogram()(waveform)\n",
        "        return spectrogram\n",
        "\n",
        "    elif transform_type == \"melspectrogram\":\n",
        "        waveform = torchaudio.tensor(audio_data)\n",
        "        melspectrogram = transforms.MelSpectrogram()(waveform)\n",
        "        return melspectrogram\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid transform_type. Please choose one of the supported transformations.\")\n",
        "\n",
        "\n",
        "def representation_transforms(audio_representation, transform_type=\"crop\", **kwargs):\n",
        "    if transform_type == \"crop\":\n",
        "        transformed_representation = transforms.Crop(**kwargs)(audio_representation)\n",
        "\n",
        "    elif transform_type == \"resize\":\n",
        "        transformed_representation = transforms.Resize(**kwargs)(audio_representation)\n",
        "\n",
        "    elif transform_type == \"add_noise\":\n",
        "        transformed_representation = transforms.AdditiveNoise(**kwargs)(audio_representation)\n",
        "\n",
        "    elif transform_type == \"adjust_contrast\":\n",
        "        transformed_representation = transforms.Contrast(**kwargs)(audio_representation)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid transform_type. Please choose one of the supported transformations.\")\n",
        "\n",
        "    return transformed_representation\n"
      ],
      "metadata": {
        "id": "kVgJj0PhT-nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Returns the dataset size.\n",
        "    NOTE - handle self.iteration_size correctly\n",
        "    \"\"\"\n",
        "    return len(self.data_path)\n"
      ],
      "metadata": {
        "id": "F_0n_UDXVdai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VoiceDataset(tc.utils.data.Dataset):\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Loads given case and returns it with the appropriate representation.\n",
        "        For raw data: [CxL], where C is the number of channels, L is the number of samples\n",
        "        For spectrogram/melspectrogram data: [CxYxX], where C is the number of channels, Y is the number of rows, X is the number of columns\n",
        "\n",
        "        The inputs will then collated to [BxCxL] or [BxCxYxX] by the collating function during loading (does not need to be defined now).\n",
        "        \"\"\"\n",
        "        audio_file_path = self.data_path[idx]\n",
        "        audio_data, sample_rate = sf.read(audio_file_path)\n",
        "\n",
        "        if self.raw_transforms is not None:\n",
        "            audio_data = self.raw_transforms(audio_data)\n",
        "\n",
        "        metadata = None\n",
        "        if self.return_metadata:\n",
        "            metadata = self.get_metadata(idx)\n",
        "\n",
        "        ground_truth = None\n",
        "        if self.return_ground_truth:\n",
        "            ground_truth = self.get_ground_truth(idx)\n",
        "\n",
        "        if self.representation_transforms is not None:\n",
        "            audio_data = self.representation_transforms(audio_data)\n",
        "\n",
        "        if self.ground_truth_mapper is not None and ground_truth is not None:\n",
        "            ground_truth = self.ground_truth_mapper.get(ground_truth, ground_truth)\n",
        "\n",
        "        # Perform collation here\n",
        "        if len(audio_data.shape) == 2: \n",
        "            audio_data = audio_data.unsqueeze(0)  \n",
        "        elif len(audio_data.shape) == 3:  \n",
        "            audio_data = audio_data.unsqueeze(0)  \n",
        "        data_tuple = (audio_data, metadata, ground_truth)\n",
        "\n",
        "        return data_tuple\n"
      ],
      "metadata": {
        "id": "9spxgfQJV3GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OSTATECZNA WERSJA:"
      ],
      "metadata": {
        "id": "E527EFxEYIxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "import torchaudio.transforms as transforms\n",
        "from typing import Union, Callable\n",
        "import torch as tc\n",
        "import pathlib\n",
        "from enum import Enum\n",
        "\n",
        "class VoiceLoadMode(Enum):\n",
        "    RAW = 1\n",
        "    SPECTROGRAM = 2\n",
        "    MELSPECTROGRAM = 3\n",
        "\n",
        "class VoiceDataset(tc.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path: Union[str, pathlib.Path],\n",
        "        csv_path: Union[str, pathlib.Path],\n",
        "        load_mode: VoiceLoadMode,\n",
        "        loading_params: dict = {},\n",
        "        raw_transforms: Callable = None,\n",
        "        representation_transforms: Callable = None,\n",
        "        return_metadata: bool = False,\n",
        "        return_ground_truth: bool = False,\n",
        "        ground_truth_mapper: dict = None\n",
        "    ):\n",
        "        self.data_path = data_path\n",
        "        self.csv_path = csv_path\n",
        "        self.load_mode = load_mode\n",
        "        self.loading_params = loading_params\n",
        "        self.raw_transforms = raw_transforms\n",
        "        self.representation_transforms = representation_transforms\n",
        "        self.return_metadata = return_metadata\n",
        "        self.return_ground_truth = return_ground_truth\n",
        "        self.ground_truth_mapper = ground_truth_mapper\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_file_path = self.data_path[idx]\n",
        "        audio_data, sample_rate = sf.read(audio_file_path)\n",
        "\n",
        "        if self.raw_transforms is not None:\n",
        "            audio_data = self.raw_transforms(audio_data)\n",
        "\n",
        "        metadata = None\n",
        "        if self.return_metadata:\n",
        "            metadata = self.get_metadata(idx)\n",
        "\n",
        "        ground_truth = None\n",
        "        if self.return_ground_truth:\n",
        "            ground_truth = self.get_ground_truth(idx)\n",
        "\n",
        "        if self.representation_transforms is not None:\n",
        "            audio_data = self.representation_transforms(audio_data)\n",
        "\n",
        "        if self.ground_truth_mapper is not None and ground_truth is not None:\n",
        "            ground_truth = self.ground_truth_mapper.get(ground_truth, ground_truth)\n",
        "\n",
        "        if self.return_metadata and self.return_ground_truth:\n",
        "            return audio_data, metadata, ground_truth\n",
        "        elif self.return_metadata:\n",
        "            return audio_data, metadata\n",
        "        elif self.return_ground_truth:\n",
        "            return audio_data, ground_truth\n",
        "        else:\n",
        "            return audio_data\n",
        "\n",
        "    def get_metadata(self, index):\n",
        "        metadata_df = pd.read_csv(self.csv_path)\n",
        "        row = metadata_df.iloc[index]\n",
        "        metadata = row.to_dict()\n",
        "        return metadata\n",
        "\n",
        "    def get_ground_truth(self, index):\n",
        "        metadata_df = pd.read_csv(self.csv_path)\n",
        "        ground_truth = metadata_df.iloc[index][\"label\"]\n",
        "        return ground_truth\n"
      ],
      "metadata": {
        "id": "7QGYLuudV8_r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}